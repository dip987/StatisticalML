{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Out Perceptrons\n",
    "\n",
    "In this project, we tested out different encoding and imputation schemes using a much simpler data regressor. Now we use the best methods found from there onto a perceptron to get even better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Starting up anything, we need to add the folder containing all the source code to Jupyter Notebooks\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\project_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from missingdata import DataImputer\n",
    "from exploration_helper_functions import *\n",
    "import seaborn as sns\n",
    "from encoding import DataEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# Loading up and Preparing data\n",
    "data_path = Path(r'data/netflix_data.csv')\n",
    "df = load_data(data_path)\n",
    "\n",
    "imputer = DataImputer()\n",
    "imputer.fit_transform(df)\n",
    "encoder = DataEncoder()\n",
    "x, y = encoder.fit_transform(dataframe=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Out Different Models\n",
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "335/335 [==============================] - 2s 5ms/step - loss: 0.0022 - val_loss: 0.0080\n",
      "Epoch 2/30\n",
      "335/335 [==============================] - 1s 1ms/step - loss: 7.4064e-04 - val_loss: 0.0062\n",
      "Epoch 3/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 6.2818e-04 - val_loss: 0.0065\n",
      "Epoch 4/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 5.3246e-04 - val_loss: 0.0048\n",
      "Epoch 5/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 5.7972e-04 - val_loss: 0.0055\n",
      "Epoch 6/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 3.5270e-04 - val_loss: 0.0043\n",
      "Epoch 7/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 4.5349e-04 - val_loss: 0.0051\n",
      "Epoch 8/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 4.1233e-04 - val_loss: 0.0057\n",
      "Epoch 9/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 3.3400e-04 - val_loss: 0.0048\n",
      "Epoch 10/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 3.3153e-04 - val_loss: 0.0051\n",
      "Epoch 11/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.7801e-04 - val_loss: 0.0055\n",
      "Epoch 12/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.5206e-04 - val_loss: 0.0057\n",
      "Epoch 13/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.4594e-04 - val_loss: 0.0051\n",
      "Epoch 14/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.2048e-04 - val_loss: 0.0052\n",
      "Epoch 15/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.2936e-04 - val_loss: 0.0062\n",
      "Epoch 16/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.3703e-04 - val_loss: 0.0052\n",
      "Epoch 17/30\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 1.8273e-04 - val_loss: 0.0049\n",
      "Epoch 18/30\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 2.2767e-04 - val_loss: 0.0053\n",
      "Epoch 19/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.8045e-04 - val_loss: 0.0054\n",
      "Epoch 20/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.6932e-04 - val_loss: 0.0052\n",
      "Epoch 21/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.5205e-04 - val_loss: 0.0052\n",
      "Epoch 22/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.6784e-04 - val_loss: 0.0055\n",
      "Epoch 23/30\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 1.6268e-04 - val_loss: 0.0051\n",
      "Epoch 24/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.6271e-04 - val_loss: 0.0055\n",
      "Epoch 25/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.5391e-04 - val_loss: 0.0055\n",
      "Epoch 26/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.6278e-04 - val_loss: 0.0054\n",
      "Epoch 27/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.6833e-04 - val_loss: 0.0053\n",
      "Epoch 28/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.2636e-04 - val_loss: 0.0050\n",
      "Epoch 29/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.3681e-04 - val_loss: 0.0054\n",
      "Epoch 30/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.2683e-04 - val_loss: 0.0052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29e43afa4f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = tf.keras.models.Sequential([tf.keras.layers.Dense(16,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(16,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(8,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(8,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(4,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(1)])\n",
    "model1.compile(optimizer='adam', loss='mse')\n",
    "model1.fit(x.toarray(), y, epochs=30, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gives us a training MSE of 1.26e-4 and validation MSE 5.2e-3. Both of which are much lower the loss from our previous XGBoost regressor.\n",
    "\n",
    "### Model 2\n",
    "Using Softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "335/335 [==============================] - 2s 4ms/step - loss: 0.0076 - val_loss: 0.0100\n",
      "Epoch 2/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 0.0012 - val_loss: 0.0100\n",
      "Epoch 3/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.0101\n",
      "Epoch 4/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.0098\n",
      "Epoch 5/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.0095\n",
      "Epoch 6/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 9.9082e-04 - val_loss: 0.0079\n",
      "Epoch 7/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 7.9313e-04 - val_loss: 0.0068\n",
      "Epoch 8/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 8.4252e-04 - val_loss: 0.0060\n",
      "Epoch 9/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 6.9321e-04 - val_loss: 0.0057\n",
      "Epoch 10/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 5.6512e-04 - val_loss: 0.0052\n",
      "Epoch 11/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 5.3166e-04 - val_loss: 0.0054\n",
      "Epoch 12/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 4.9064e-04 - val_loss: 0.0047\n",
      "Epoch 13/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 4.2373e-04 - val_loss: 0.0047\n",
      "Epoch 14/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 4.4008e-04 - val_loss: 0.0046\n",
      "Epoch 15/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 3.8737e-04 - val_loss: 0.0049\n",
      "Epoch 16/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 4.1325e-04 - val_loss: 0.0046\n",
      "Epoch 17/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 3.5833e-04 - val_loss: 0.0047\n",
      "Epoch 18/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 3.6537e-04 - val_loss: 0.0048\n",
      "Epoch 19/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 3.6815e-04 - val_loss: 0.0049\n",
      "Epoch 20/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 3.6255e-04 - val_loss: 0.0048\n",
      "Epoch 21/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 3.6610e-04 - val_loss: 0.0050\n",
      "Epoch 22/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 3.6440e-04 - val_loss: 0.0049\n",
      "Epoch 23/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 3.3040e-04 - val_loss: 0.0047\n",
      "Epoch 24/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 3.4666e-04 - val_loss: 0.0050\n",
      "Epoch 25/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 3.8721e-04 - val_loss: 0.0049\n",
      "Epoch 26/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.6239e-04 - val_loss: 0.0050\n",
      "Epoch 27/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.9835e-04 - val_loss: 0.0051\n",
      "Epoch 28/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.8815e-04 - val_loss: 0.0050\n",
      "Epoch 29/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.9188e-04 - val_loss: 0.0049\n",
      "Epoch 30/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.5468e-04 - val_loss: 0.0049\n",
      "Epoch 31/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.4972e-04 - val_loss: 0.0048\n",
      "Epoch 32/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.6927e-04 - val_loss: 0.0050\n",
      "Epoch 33/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.9037e-04 - val_loss: 0.0050\n",
      "Epoch 34/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.2358e-04 - val_loss: 0.0049\n",
      "Epoch 35/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 2.8619e-04 - val_loss: 0.0049\n",
      "Epoch 36/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.5141e-04 - val_loss: 0.0048\n",
      "Epoch 37/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.7577e-04 - val_loss: 0.0052\n",
      "Epoch 38/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.6259e-04 - val_loss: 0.0048\n",
      "Epoch 39/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.4605e-04 - val_loss: 0.0049\n",
      "Epoch 40/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.1811e-04 - val_loss: 0.0049\n",
      "Epoch 41/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.5716e-04 - val_loss: 0.0050\n",
      "Epoch 42/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 2.4157e-04 - val_loss: 0.0051\n",
      "Epoch 43/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 2.5101e-04 - val_loss: 0.0052\n",
      "Epoch 44/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.6809e-04 - val_loss: 0.0050\n",
      "Epoch 45/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.2336e-04 - val_loss: 0.0051\n",
      "Epoch 46/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.4534e-04 - val_loss: 0.0052\n",
      "Epoch 47/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.2388e-04 - val_loss: 0.0052\n",
      "Epoch 48/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.5426e-04 - val_loss: 0.0052\n",
      "Epoch 49/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.1557e-04 - val_loss: 0.0050\n",
      "Epoch 50/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.2121e-04 - val_loss: 0.0054\n",
      "Epoch 51/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.9834e-04 - val_loss: 0.0053\n",
      "Epoch 52/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.0411e-04 - val_loss: 0.0051\n",
      "Epoch 53/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.3755e-04 - val_loss: 0.0051\n",
      "Epoch 54/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.0215e-04 - val_loss: 0.0051\n",
      "Epoch 55/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.9104e-04 - val_loss: 0.0054\n",
      "Epoch 56/60\n",
      "335/335 [==============================] - 1s 3ms/step - loss: 2.0776e-04 - val_loss: 0.0052\n",
      "Epoch 57/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.3421e-04 - val_loss: 0.0052\n",
      "Epoch 58/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.0281e-04 - val_loss: 0.0053\n",
      "Epoch 59/60\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.7977e-04 - val_loss: 0.0052\n",
      "Epoch 60/60\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.8853e-04 - val_loss: 0.0053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29ecd6326a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = tf.keras.models.Sequential([tf.keras.layers.Dense(16,  activation='softmax'),\n",
    "                                    tf.keras.layers.Dense(16,  activation='softmax'),\n",
    "                                    tf.keras.layers.Dense(8,  activation='softmax'),\n",
    "                                    tf.keras.layers.Dense(8,  activation='softmax'),\n",
    "                                    tf.keras.layers.Dense(4,  activation='softmax'),\n",
    "                                    tf.keras.layers.Dense(1)])\n",
    "model1.compile(optimizer='adam', loss='mse')\n",
    "model1.fit(x.toarray(), y, epochs=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gives us a training MSE of 1.88e-4 and validation MSE 5.3e-3. which pretty comparable to our previous model although this time it took a more epochs to converge.\n",
    "\n",
    "### Model 3\n",
    "A deeper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "335/335 [==============================] - 2s 3ms/step - loss: 9.5490e-04 - val_loss: 0.0064\n",
      "Epoch 2/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 6.2477e-04 - val_loss: 0.0062\n",
      "Epoch 3/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 5.1381e-04 - val_loss: 0.0052\n",
      "Epoch 4/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 3.4450e-04 - val_loss: 0.0052\n",
      "Epoch 5/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 4.9071e-04 - val_loss: 0.0046\n",
      "Epoch 6/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.5206e-04 - val_loss: 0.0053\n",
      "Epoch 7/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.0147e-04 - val_loss: 0.0058\n",
      "Epoch 8/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.1944e-04 - val_loss: 0.0052\n",
      "Epoch 9/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 2.0303e-04 - val_loss: 0.0062\n",
      "Epoch 10/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.9234e-04 - val_loss: 0.0049\n",
      "Epoch 11/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.6230e-04 - val_loss: 0.0049\n",
      "Epoch 12/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.6035e-04 - val_loss: 0.0050\n",
      "Epoch 13/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.2137e-04 - val_loss: 0.0050\n",
      "Epoch 14/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.2790e-04 - val_loss: 0.0048\n",
      "Epoch 15/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.0984e-04 - val_loss: 0.0050\n",
      "Epoch 16/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.1453e-04 - val_loss: 0.0046\n",
      "Epoch 17/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.1018e-04 - val_loss: 0.0051\n",
      "Epoch 18/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 9.3633e-05 - val_loss: 0.0050\n",
      "Epoch 19/40\n",
      "335/335 [==============================] - 1s 1ms/step - loss: 9.7867e-05 - val_loss: 0.0052\n",
      "Epoch 20/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.0579e-04 - val_loss: 0.0048\n",
      "Epoch 21/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.0070e-04 - val_loss: 0.0048\n",
      "Epoch 22/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 9.3625e-05 - val_loss: 0.0047\n",
      "Epoch 23/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 8.7105e-05 - val_loss: 0.0048\n",
      "Epoch 24/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 1.0454e-04 - val_loss: 0.0051\n",
      "Epoch 25/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 7.0789e-05 - val_loss: 0.0049\n",
      "Epoch 26/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 8.3576e-05 - val_loss: 0.0049\n",
      "Epoch 27/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 7.1793e-05 - val_loss: 0.0049\n",
      "Epoch 28/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 6.9889e-05 - val_loss: 0.0048\n",
      "Epoch 29/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 6.2701e-05 - val_loss: 0.0050\n",
      "Epoch 30/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 6.9540e-05 - val_loss: 0.0050\n",
      "Epoch 31/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 6.7881e-05 - val_loss: 0.0048\n",
      "Epoch 32/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 6.2016e-05 - val_loss: 0.0046\n",
      "Epoch 33/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 5.8958e-05 - val_loss: 0.0049\n",
      "Epoch 34/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 8.3650e-05 - val_loss: 0.0051\n",
      "Epoch 35/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 5.8490e-05 - val_loss: 0.0048\n",
      "Epoch 36/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 5.5791e-05 - val_loss: 0.0046\n",
      "Epoch 37/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 5.0926e-05 - val_loss: 0.0052\n",
      "Epoch 38/40\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 5.5471e-05 - val_loss: 0.0047\n",
      "Epoch 39/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 7.0200e-05 - val_loss: 0.0050\n",
      "Epoch 40/40\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 6.9632e-05 - val_loss: 0.0050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29ecc0675e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = tf.keras.models.Sequential([tf.keras.layers.Dense(64,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(64,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(32,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(32,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(16,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(16,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(8,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(8,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(4,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(1)])\n",
    "model1.compile(optimizer='adam', loss='mse')\n",
    "model1.fit(x.toarray(), y, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gives us a training MSE of 7e-5 and validation MSE 5e-3. Which is also very comparable to the previous to models(At least in terms of validation loss). So increasing layers did not give us any performance advantage.\n",
    "\n",
    "### Model 4\n",
    "using tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 0.0037 - val_loss: 0.0078\n",
      "Epoch 2/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 8.2166e-04 - val_loss: 0.0069\n",
      "Epoch 3/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 6.8970e-04 - val_loss: 0.0062\n",
      "Epoch 4/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 6.3654e-04 - val_loss: 0.0062\n",
      "Epoch 5/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 5.2147e-04 - val_loss: 0.0058\n",
      "Epoch 6/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 4.9571e-04 - val_loss: 0.0056\n",
      "Epoch 7/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 5.2735e-04 - val_loss: 0.0054\n",
      "Epoch 8/30\n",
      "335/335 [==============================] - 1s 2ms/step - loss: 4.6451e-04 - val_loss: 0.0051\n",
      "Epoch 9/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 4.0909e-04 - val_loss: 0.0054\n",
      "Epoch 10/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 3.5966e-04 - val_loss: 0.0055\n",
      "Epoch 11/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 4.3817e-04 - val_loss: 0.0053\n",
      "Epoch 12/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 3.1444e-04 - val_loss: 0.0059\n",
      "Epoch 13/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.8327e-04 - val_loss: 0.0057\n",
      "Epoch 14/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 3.0858e-04 - val_loss: 0.0052\n",
      "Epoch 15/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.5222e-04 - val_loss: 0.0052\n",
      "Epoch 16/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.2913e-04 - val_loss: 0.0055\n",
      "Epoch 17/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.4775e-04 - val_loss: 0.0050\n",
      "Epoch 18/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.6067e-04 - val_loss: 0.0054\n",
      "Epoch 19/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.3527e-04 - val_loss: 0.0054\n",
      "Epoch 20/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.0606e-04 - val_loss: 0.0051\n",
      "Epoch 21/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.0653e-04 - val_loss: 0.0062\n",
      "Epoch 22/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.8288e-04 - val_loss: 0.0051\n",
      "Epoch 23/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.8177e-04 - val_loss: 0.0055\n",
      "Epoch 24/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.3049e-04 - val_loss: 0.0053\n",
      "Epoch 25/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.6327e-04 - val_loss: 0.0052\n",
      "Epoch 26/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.8755e-04 - val_loss: 0.0049\n",
      "Epoch 27/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.7043e-04 - val_loss: 0.0049\n",
      "Epoch 28/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 2.0022e-04 - val_loss: 0.0049\n",
      "Epoch 29/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.8929e-04 - val_loss: 0.0049\n",
      "Epoch 30/30\n",
      "335/335 [==============================] - 0s 1ms/step - loss: 1.4825e-04 - val_loss: 0.0051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29ecb452b80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = tf.keras.models.Sequential([tf.keras.layers.Dense(16,  activation='relu'),\n",
    "                                    tf.keras.layers.Dense(16,  activation='tanh'),\n",
    "                                    tf.keras.layers.Dense(8,  activation='tanh'),\n",
    "                                    tf.keras.layers.Dense(8,  activation='tanh'),\n",
    "                                    tf.keras.layers.Dense(4,  activation='tanh'),\n",
    "                                    tf.keras.layers.Dense(1)])\n",
    "model1.compile(optimizer='adam', loss='mse')\n",
    "model1.fit(x.toarray(), y, epochs=30, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has 1.4e-4 training loss and 5.1e-3 validation loss which is also very comparable to the previous ones.\n",
    "\n",
    "## Conclusion\n",
    "Using a perceptron with 6 layers gives us a better performance than the XGBoost algortihms we usedd previously. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
